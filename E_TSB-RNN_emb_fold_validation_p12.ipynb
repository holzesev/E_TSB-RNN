{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "# Seed value\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "del seed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Input: We load both datasets (dirty and clean) as dirty_table and clean_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input(data):\n",
    "    # Load Data\n",
    "    dirty_table = pd.read_csv('./datasets/' + data + '/dirty.csv', sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str, keep_default_na=False, low_memory=False)\n",
    "    clean_table = pd.read_csv('./datasets/' + data + '/clean.csv', sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str, keep_default_na=False, low_memory=False)\n",
    "\n",
    "    return dirty_table, clean_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Structure Transformation: Next we rename the column names in the dirty_table to have identical names with the clean dataset. We need this to combine the information of both datasets and create a new one (df). Also we add tid as sequence number for every row. At the end we cute the strings after 100 characters (numcharmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Structure(Tablestructure_equal, dirty_table, clean_table):\n",
    "    # Structure dirty_table and clean_table equal? (names of columns can be different)\n",
    "    #Tablestructure_equal = True\n",
    "\n",
    "    # Rename the different columnames\n",
    "    cols_clean_table = list(clean_table.columns.values)\n",
    "    cols_dirty_table = list(dirty_table.columns.values)\n",
    "\n",
    "    if sorted(cols_clean_table) == sorted(cols_dirty_table): \n",
    "        print (\"The lists are identical\")\n",
    "    else : \n",
    "        print (\"The lists are not identical\")\n",
    "        if Tablestructure_equal == True:\n",
    "            print (\"The dirty and clean have the same structure. We use the columnames from clean for dirty.\") \n",
    "            dirty_table.columns = cols_clean_table\n",
    "\n",
    "    # Add id_\n",
    "    clean_table.insert(0, 'id_', clean_table.index)\n",
    "    clean_table = clean_table.set_index('id_')\n",
    "\n",
    "    dirty_table.insert(0, 'id_', dirty_table.index)\n",
    "    dirty_table = dirty_table.set_index('id_')\n",
    "\n",
    "    dirty_table = dirty_table.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    dirty_table = dirty_table.fillna('')\n",
    "    clean_table = clean_table.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    clean_table = clean_table.fillna('')\n",
    "\n",
    "    # Generate table attribute with information about columns\n",
    "    attribute = pd.DataFrame(clean_table.columns.to_numpy(), columns = ['name'])\n",
    "    measurer = np.vectorize(len)\n",
    "    attribute['maxnumchar1'] = measurer(dirty_table.astype(str)).max(axis=0)\n",
    "    attribute['maxnumchar']=np.where(attribute['maxnumchar1']>128, 128, attribute['maxnumchar1'])\n",
    "\n",
    "    maxlen = np.max(attribute['maxnumchar'])\n",
    "    print(\"Maximum value_x length: \", maxlen)\n",
    "\n",
    "    return dirty_table, clean_table, attribute, maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Merge: Next we combine the two tables in the dataset df where every cell of the dirty_table / clean_table is saved in the columns value_x / value_y, respectively. For the models we need an attribute value, i.e. a label, which includes 0 (correct) or 1 (wrong). We get this value when comparing value_x and value_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(dirty_table, clean_table):\n",
    "    # Produce datasets which transformed the table in rows\n",
    "    clean_row=clean_table.unstack().reset_index()\n",
    "    clean_row['Sort'] = clean_row.index\n",
    "    clean_row = clean_row.rename(columns={'level_0':'attribute','level_1':'id_',0:'value'}).sort_values(by=['id_','Sort'])\n",
    "    clean_row=clean_row.reset_index(drop=True).drop(columns='Sort')\n",
    "\n",
    "    dirty_row=dirty_table.unstack().reset_index()\n",
    "    dirty_row['Sort'] = dirty_row.index\n",
    "    dirty_row = dirty_row.rename(columns={'level_0':'attribute','level_1':'id_',0:'value'}).sort_values(by=['id_','Sort'])\n",
    "    dirty_row=dirty_row.reset_index(drop=True).drop(columns='Sort')\n",
    "\n",
    "    # Produce datasets for M2\n",
    "    X_roh = dirty_table\n",
    "    y = clean_table != dirty_table\n",
    "\n",
    "    y = y.astype(int)\n",
    "\n",
    "    # Merge datasets together\n",
    "    df = pd.merge(dirty_row, clean_row, on=['id_', \"attribute\"])\n",
    "\n",
    "    # Show rows which are empty (1)\n",
    "    df['empty1'] = np.where(np.isin(df['value_x'].str.lower(),['', 'nan','n/a','n/n']) == True,1,0)\n",
    "\n",
    "    # Compare content of dirty and clean dataset\n",
    "    df['value'] = np.where(df['value_x'] == df['value_y'], 0, 1)\n",
    "\n",
    "    # Concatenate attributename and value_x (dirty)\n",
    "    df['concat'] = df['attribute'] + '_' + df['value_x']\n",
    "\n",
    "    df['length'] = df.value_x.str.len()\n",
    "\n",
    "    Summe = df.groupby('value')['id_'].count()\n",
    "    print(Summe)\n",
    "    print()\n",
    "    print('Error Rate:'+ str(round(1/(Summe[0]+Summe[1])*Summe[1],2)))\n",
    "\n",
    "    return df, X_roh, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Dictionary Generation: Before we can feed the data into a neural network, we need to transform the data types from character to numeric character embedding. We produce a value dictionary (char_index) which contains an index for each character in value_x.\n",
    "\n",
    "For the ETSB-RNN we also need an attribute dictionary (attribute_index) which includes an index for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dictionary(attribute,df):\n",
    "    # Tokenizer character\n",
    "    tk_char = tf.keras.preprocessing.text.Tokenizer(num_words=False, lower=False, char_level=True)\n",
    "    tk_char.fit_on_texts(df.value_x)\n",
    "    tk_char_len=len(tk_char.word_index)\n",
    "    print(\"Number of characters: \" + str(tk_char_len))\n",
    "    print(tk_char.word_index)\n",
    "    #tk_char_list = list(tk_char.word_index.keys())\n",
    "\n",
    "    # Tokenizer attribute\n",
    "    tk_attr = tf.keras.preprocessing.text.Tokenizer(num_words=False, filters='', lower=False, char_level=False, split=\"nosplit\")\n",
    "    tk_attr.fit_on_texts(df.attribute)\n",
    "    print(\"Number of attributs: \" + str(len(tk_attr.word_index)))\n",
    "    print(tk_attr.word_index)\n",
    "\n",
    "    return tk_char, tk_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_extend(attribute,df,X_roh,y):\n",
    "    # Print properties (length and number of errors per column)\n",
    "    i=0\n",
    "    num_error_col=0\n",
    "    Drop_list = []\n",
    "    for attr in attribute['name']:\n",
    "        df2 = df[df['attribute']==attr]\n",
    "        maxnumchar = attribute.loc[attribute['name']==attr]['maxnumchar'].to_numpy()[0]\n",
    "        maxnumchar1 = attribute.loc[attribute['name']==attr]['maxnumchar1'].to_numpy()[0]\n",
    "        summe = np.sum(df2.value)\n",
    "        attribute.loc[attribute['name'] == attr, 'error'] = int(summe)\n",
    "        \n",
    "        #if maxnumchar1 > 500:\n",
    "        if maxnumchar1 > 10000:\n",
    "            Drop_list.append(attr)\n",
    "        else:\n",
    "            i=i+1\n",
    "            arr = tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(X_roh[attr].astype(str)), maxlen=maxnumchar, padding='post')\n",
    "            if i == 1:\n",
    "                X = arr\n",
    "            else:\n",
    "                X = np.concatenate((X, arr), axis=1)\n",
    "                \n",
    "        tk_perCol = tf.keras.preprocessing.text.Tokenizer(num_words=False, lower=False, char_level=True)\n",
    "        tk_perCol.fit_on_texts(X_roh[attr])\n",
    "        attribute.loc[attribute['name'] == attr, 'numuniquechar'] = len(tk_perCol.word_index)\n",
    "        df.loc[df['attribute'] == attr, 'length_norm'] = df['length']/maxnumchar1\n",
    "\n",
    "        print(attr)\n",
    "        print('Max lenght: ' + str(maxnumchar1) + ' --> ' + str(maxnumchar))\n",
    "        print('Unique characters: ' + str(len(tk_perCol.word_index)))\n",
    "        #print(tk_perCol.word_index)\n",
    "        print('Number of errors: ' + str(summe))\n",
    "        print('')\n",
    "        if summe > 0:\n",
    "            num_error_col+=1\n",
    "\n",
    "    print(str(num_error_col) + '/' + str(len(attribute)) + ' faulty attributes')\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "    X.insert(0, 'id_', X.index)\n",
    "    X = X.set_index('id_')\n",
    "    y = pd.DataFrame(y.drop(columns=Drop_list))\n",
    "    X.reset_index(level=0, inplace=True)\n",
    "    y.reset_index(level=0, inplace=True)\n",
    "\n",
    "    return attribute, X, y, Drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chose_Model(Mod):\n",
    "    global model\n",
    "    global checkpoint_path\n",
    "    global checkpoint\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    checkpoint_path = 'checkpoint/' + data + '/p12/checkpoint_p12_' + Mod\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=ver\n",
    "    )\n",
    "    \n",
    "    if Mod=='M0':\n",
    "        # Define TSB-RNN\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\n",
    "\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\n",
    "\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(x)\n",
    "        x = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=inputA, outputs=z)\n",
    "\n",
    "    elif Mod=='M1alt':\n",
    "        # Define ETSB-RNN\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\n",
    "        inputB = tf.keras.Input(shape=(1,))\n",
    "\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\n",
    "        b = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(inputB)\n",
    "\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(x)\n",
    "        x = tf.keras.models.Model(inputs=inputA, outputs=x)\n",
    "\n",
    "        y = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=True))(b)\n",
    "        y = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=False))(y)\n",
    "        y = tf.keras.models.Model(inputs=inputB, outputs=y)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(combined)\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "    elif Mod=='M1':\n",
    "        # Define ETSB-RNN new\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\n",
    "        a = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\n",
    "        a = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(a)\n",
    "\n",
    "        inputB = tf.keras.Input(shape=(1,))\n",
    "        b = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(inputB)\n",
    "        b = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=True))(b)\n",
    "        b = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=False))(b)\n",
    "\n",
    "        inputC = tf.keras.Input(shape=(1,))\n",
    "        c = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(inputC)\n",
    "        c = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(c)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([a, b, c])\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(combined)\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[inputA, inputB, inputC], outputs=z)\n",
    "\n",
    "    elif Mod=='M2':\n",
    "        # Define MTSB-RNN\n",
    "        crop1=0\n",
    "        width_1=X.shape[1]+maxlen\n",
    "        crop2=width_1\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(width_1,))\n",
    "        inputsIn = tf.keras.layers.Reshape((width_1,1))(inputs)\n",
    "        #inputsIn = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char)(inputs)\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "\n",
    "        for index, row in attribute.iterrows():\n",
    "            numuniquechar=int(row['numuniquechar']+1)\n",
    "            maxnumchar=row['maxnumchar']\n",
    "            In = 'In'+str(index)\n",
    "            crop2-=maxnumchar\n",
    "\n",
    "            In = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\n",
    "            In = tf.keras.layers.Reshape((maxnumchar,), input_shape=(maxnumchar,1))(In)\n",
    "            In = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In)\n",
    "            In = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=True))(In)\n",
    "            In = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=False))(In)\n",
    "            all_inputs.append(In)\n",
    "\n",
    "            crop1+=maxnumchar\n",
    "\n",
    "        crop2-=maxlen\n",
    "        In_value = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\n",
    "        In_value = tf.keras.layers.Reshape((maxlen,), input_shape=(maxlen,1))(In_value)\n",
    "        In_value = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In_value)\n",
    "        In_value = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=True))(In_value)\n",
    "        In_value = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=False))(In_value)\n",
    "        all_inputs.append(In_value)\n",
    "        crop1+=maxlen\n",
    "\n",
    "        crop2-=1\n",
    "        In_attr = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\n",
    "        In_attr = tf.keras.layers.Reshape((1,), input_shape=(1,1))(In_attr)\n",
    "        In_attr = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(In_attr)\n",
    "        In_attr = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim_att)), return_sequences=True))(In_attr)\n",
    "        In_attr = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim_att)), return_sequences=False))(In_attr)\n",
    "        all_inputs.append(In_attr)\n",
    "        crop1+=1\n",
    "\n",
    "        combined = tf.keras.layers.concatenate(all_inputs)\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(combined)\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\n",
    "\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=z)\n",
    "\n",
    "    elif Mod=='M3':\n",
    "        crop1=0\n",
    "        width_1=X.shape[1]-1\n",
    "        crop2=width_1\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(width_1,))\n",
    "        inputsIn = tf.keras.layers.Reshape((width_1,1))(inputs)\n",
    "        all_inputs1 = []\n",
    "        all_inputs2 = []\n",
    "        all_outputs = []\n",
    "\n",
    "        for index, row in attribute.iterrows():\n",
    "            numuniquechar=int(row['numuniquechar']+1)\n",
    "            maxnumchar=row['maxnumchar']\n",
    "            In = 'In'+str(index)\n",
    "            crop2-=maxnumchar\n",
    "            \n",
    "            In = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\n",
    "            In = tf.keras.layers.Reshape((maxnumchar,), input_shape=(maxnumchar,1))(In)\n",
    "            In = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In)\n",
    "\n",
    "            In1 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round(numuniquechar/2), return_sequences=True))(In)\n",
    "            In1 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round(numuniquechar/2)))(In1)\n",
    "            In1 = tf.keras.layers.Dense(numuniquechar, activation=\"relu\")(In1)\n",
    "            In1 = tf.keras.layers.Dropout(0.1)(In1)\n",
    "\n",
    "            In2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((emb_dim_char)/2), return_sequences=True))(In)\n",
    "            In2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((emb_dim_char)/2)))(In2)\n",
    "            In2 = tf.keras.layers.Dense(emb_dim_char, activation=\"relu\")(In2)\n",
    "            In2 = tf.keras.layers.Dropout(0.1)(In2)\n",
    "\n",
    "            all_inputs1.append(In1)\n",
    "            all_inputs2.append(In2)\n",
    "\n",
    "            crop1+=maxnumchar\n",
    "\n",
    "        combined1 = tf.keras.layers.concatenate(all_inputs1)\n",
    "        combined1 = tf.keras.layers.Dense(round(combined1.shape[1]/2), activation=\"relu\")(combined1)\n",
    "        combined1 = tf.keras.layers.Dropout(0.1)(combined1)\n",
    "\n",
    "        combined2 = tf.keras.layers.concatenate(all_inputs2)\n",
    "        combined2 = tf.keras.layers.Dense(emb_dim_char, activation=\"relu\")(combined2)\n",
    "        combined2 = tf.keras.layers.Dropout(0.1)(combined2)\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([combined1, combined2])\n",
    "\n",
    "        for index, row in attribute.iterrows():\n",
    "            Out = 'Out'+str(index)\n",
    "            Out = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "            all_outputs.append(Out)\n",
    "\n",
    "        combinedOut = tf.keras.layers.concatenate(all_outputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs,outputs=combinedOut)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    #model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raha(num_sample):\n",
    "    import pandas\n",
    "    import IPython.display\n",
    "\n",
    "    import raha\n",
    "    \n",
    "    app_1 = raha.Detection()\n",
    "    app_1.LABELING_BUDGET = num_sample\n",
    "\n",
    "    dataset_dictionary = {\n",
    "        \"name\": data,\n",
    "        \"path\": './datasets/' + data + '/dirty.csv',\n",
    "        \"clean_path\": './datasets/' + data + '/clean.csv'\n",
    "    }\n",
    "\n",
    "    d = app_1.initialize_dataset(dataset_dictionary)\n",
    "\n",
    "    app_1.run_strategies(d)\n",
    "\n",
    "    app_1.generate_features(d)\n",
    "\n",
    "    app_1.build_clusters(d)\n",
    "\n",
    "    sampled_list = []\n",
    "\n",
    "    while len(d.labeled_tuples) < app_1.LABELING_BUDGET:\n",
    "        app_1.sample_tuple(d)\n",
    "        if d.has_ground_truth:\n",
    "            app_1.label_with_ground_truth(d)\n",
    "\n",
    "        sampled_list.append(d.sampled_tuple)\n",
    "\n",
    "    return sampled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(Iteration_n,end_n,mod,sample_technique,val):\n",
    "    ID_Alle = df.groupby(['id_'], as_index=False)['value'].sum()\n",
    "    run=0\n",
    "    for n in range(20,end_n+1,10):\n",
    "        run+=1\n",
    "\n",
    "        Loss = []\n",
    "        acc = []\n",
    "        pre = []\n",
    "        rec = []\n",
    "        F1 = []\n",
    "\n",
    "        train_time = []\n",
    "        test_time = []\n",
    "        t_time = []\n",
    "        \n",
    "        for Iteration in range(1,Iteration_n+1):\n",
    "            # Start the stopwatch / counter \n",
    "            t2_start = time.process_time()\n",
    "\n",
    "            print('Test: ' + str(Iteration) + '/' + str(Iteration_n))\n",
    "            train = df[df['id_'].isin(ID_Alle['id_'])]\n",
    "            train_ID = ID_Alle['id_']\n",
    "\n",
    "            train_ID_Rest = ID_Alle['id_']\n",
    "            train_Rest = df[df['id_'].isin(train_ID_Rest)]\n",
    "            train_Rest = train_Rest[~train_Rest.attribute.isin(Drop_list)]\n",
    "\n",
    "            if sample_technique == 'RahaSet':\n",
    "                train_ID_Manuel = pd.Series(raha(n))\n",
    "                train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\n",
    "                train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\n",
    "                train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\n",
    "                train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\n",
    "\n",
    "            elif sample_technique == 'RandomSet':\n",
    "                #train_ID_Manuel = pd.Series(ID_Alle.sample(n, random_state=Iteration)['id_'])\n",
    "                train_ID_Manuel = pd.Series(ID_Alle.sample(n)['id_'])\n",
    "                train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\n",
    "                train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\n",
    "                train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\n",
    "                train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\n",
    "\n",
    "            elif sample_technique == 'DiverSet':\n",
    "                train_ID_Manuel_List = []\n",
    "\n",
    "                # Iterate for choosing the next observation\n",
    "                for i in range(0,n):\n",
    "                    # For prefering empty value_x we have to compute the number of this\n",
    "                    empty = train_Rest.groupby(['id_'])['empty1'].agg('sum')\n",
    "                    count = train_Rest.groupby(['id_']).size().to_frame()\n",
    "                    count['empty1'] = empty\n",
    "                    count = count.sort_values(by=[0,'empty1'], ascending=False)\n",
    "                    count.reset_index(inplace=True)\n",
    "                    count = count[count[0]==count[0].max()]\n",
    "                    count = count[count['empty1']==count['empty1'].max()]\n",
    "                    train_ID_Manuel_List.append(int(count.sample(1, random_state=Iteration)['id_']))\n",
    "                    train_ID_Manuel = pd.Series(train_ID_Manuel_List)\n",
    "                    train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\n",
    "                    train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\n",
    "                    train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\n",
    "                    train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\n",
    "\n",
    "                del i, count, train_ID_Manuel_List, empty\n",
    "\n",
    "            elif sample_technique == 'sev1':\n",
    "                train_ID_Manuel_List = []\n",
    "\n",
    "                # Iterate for choosing the next observation\n",
    "                for i in range(0,n):\n",
    "                    # For prefering empty value_x we have to compute the number of this\n",
    "                    empty = train_Rest.groupby(['id_'])['empty1'].agg('sum')\n",
    "                    count = train_Rest.groupby(['id_']).size().to_frame()\n",
    "                    length = train_Rest.groupby(['id_'])['length_norm'].agg('sum')\n",
    "                    count['empty1'] = empty\n",
    "                    count['length1'] = length\n",
    "                    count = count.sort_values(by=[0,'empty1','length1'], ascending=False)\n",
    "                    count.reset_index(inplace=True)\n",
    "                    count = count[count[0]==count[0].min()]\n",
    "                    count = count[count['empty1']==count['empty1'].max()]\n",
    "                    #count = count[count['length1']==count['length1'].min()]\n",
    "                    train_ID_Manuel_List.append(int(count.sample(1, random_state=Iteration)['id_']))\n",
    "                    train_ID_Manuel = pd.Series(train_ID_Manuel_List)\n",
    "                    train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\n",
    "                    train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\n",
    "                    train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\n",
    "                    train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\n",
    "\n",
    "                del i, count, train_ID_Manuel_List, empty\n",
    "\n",
    "            print('Number of train-tupels: ' + str(len(train_ID_Manuel)) + ' Sample technique: ' + str(sample_technique))\n",
    "\n",
    "            # The records which we dont need for training we use for the testing\n",
    "            test_ID = train_ID_Rest.copy()\n",
    "            #if data == 'Tax':\n",
    "            #    test_ID = test_ID.sample(10000)\n",
    "            #    print(data + ' is used! -> short version for testing')\n",
    "            #elif data == 'Movies':\n",
    "            #    test_ID = test_ID.sample(1500)\n",
    "            #    print(data + ' is used! -> short version for testing')\n",
    "            test = df[df['id_'].isin(test_ID)]\n",
    "            test = test[~test.attribute.isin(Drop_list)]\n",
    "\n",
    "            X_train_3 = np.array(X[X['id_'].isin(train_ID_Manuel)].drop(columns='id_'))\n",
    "            Y_train_3 = np.array(y[y['id_'].isin(train_ID_Manuel)].drop(columns='id_'))\n",
    "            X_test_3 = np.array(X[X['id_'].isin(test_ID)].drop(columns='id_'))\n",
    "            Y_test_3 = np.array(y[y['id_'].isin(test_ID)].drop(columns='id_'))\n",
    "\n",
    "            del train_ID, train, train_ID_Rest, train_Rest\n",
    "\n",
    "            train_Manuel_zus = pd.DataFrame()\n",
    "            train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n",
    "\n",
    "            # Transform the text to numbers\n",
    "            X_train_Manuel=pd.DataFrame(tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(train_Manuel_new.value_x), maxlen=maxlen, padding='post'))\n",
    "            X_train_1=np.array(X_train_Manuel)\n",
    "            X_train_Manuel['id_']=np.array(train_Manuel_new['id_'])\n",
    "\n",
    "            X_train_Manuel_attribute=pd.DataFrame(tk_attr.texts_to_sequences(train_Manuel_new.attribute))\n",
    "            X_train_attribute_1=np.array(X_train_Manuel_attribute)\n",
    "            X_train_Manuel_attribute['id_']=np.array(train_Manuel_new['id_'])\n",
    "\n",
    "            X_train_length_1=train_Manuel_new.length_norm.to_numpy()\n",
    "\n",
    "            Y_train=tf.keras.utils.to_categorical(train_Manuel_new.value, num_classes=2)\n",
    "            print(np.ndarray.sum(Y_train,axis=0))\n",
    "\n",
    "            X_test_Manuel=pd.DataFrame(tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(test.value_x), maxlen=maxlen, padding='post'))\n",
    "            X_test_1=np.array(X_test_Manuel)\n",
    "            X_test_Manuel['id_']=np.array(test['id_'])\n",
    "\n",
    "            X_test_Manuel_attribute=pd.DataFrame(tk_attr.texts_to_sequences(test.attribute))\n",
    "            X_test_attribute_1=np.array(X_test_Manuel_attribute)\n",
    "            X_test_Manuel_attribute['id_']=np.array(test['id_'])\n",
    "\n",
    "            X_test_length_1=test.length_norm.to_numpy()\n",
    "\n",
    "            Y_test=tf.keras.utils.to_categorical(test.value, num_classes=2)\n",
    "\n",
    "            X_train=pd.merge(X, X_train_Manuel, on=['id_'])\n",
    "            X_train['attr']=X_train_Manuel_attribute[0]\n",
    "            X_train=np.array(X_train.drop(columns='id_'))\n",
    "\n",
    "            X_test=pd.merge(X, X_test_Manuel, on=['id_'])\n",
    "            X_test['attr']=X_test_Manuel_attribute[0]\n",
    "            X_test=np.array(X_test.drop(columns='id_'))\n",
    "            \n",
    "            chose_Model(mod)\n",
    "            # Train TSB-RNN (M0)\n",
    "            if mod == 'M0':\n",
    "                if val == True:\n",
    "                    log = model.fit(X_train_1, Y_train, validation_data=(X_test_1, Y_test), shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "                    log_test_loss = log.history['val_loss']\n",
    "                    log_train_loss = log.history['loss']\n",
    "                    log_test_accuracy = log.history['val_accuracy']\n",
    "                    log_train_accuracy = log.history['accuracy']\n",
    "                else:\n",
    "                    log = model.fit(X_train_1, Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "            # Train ETSB-RNN (M1)\n",
    "            elif mod == 'M1':\n",
    "                if val == True:\n",
    "                    log = model.fit(x=[X_train_1,X_train_attribute_1,X_train_length_1], y=Y_train, validation_data=([X_test_1,X_test_attribute_1,X_test_length_1], Y_test), shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "                    log_test_loss = log.history['val_loss']\n",
    "                    log_train_loss = log.history['loss']\n",
    "                    log_test_accuracy = log.history['val_accuracy']\n",
    "                    log_train_accuracy = log.history['accuracy']\n",
    "                else:\n",
    "                    log = model.fit(x=[X_train_1,X_train_attribute_1,X_train_length_1], y=Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "            elif mod == 'M2':\n",
    "                log = model.fit(x=X_train, y=Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "            elif mod == 'M3':\n",
    "                log = model.fit(x=X_train_3, y=Y_train_3, shuffle=True, batch_size=batch_size_3, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Stop the stopwatch / counter\n",
    "            t2_stop = time.process_time()\n",
    "            t2_time = t2_stop-t2_start\n",
    "\n",
    "            # Start the stopwatch / counter \n",
    "            t3_start = time.process_time()\n",
    "\n",
    "            # Load best weights\n",
    "            model.load_weights(checkpoint_path)\n",
    "\n",
    "            # Evaluate with testsets\n",
    "            if mod == 'M0':\n",
    "                scores = model.evaluate(X_test_1, Y_test, verbose=ver)\n",
    "                Y_pred = model.predict(X_test_1)\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\n",
    "            elif mod == 'M1':\n",
    "                scores = model.evaluate([X_test_1,X_test_attribute_1,X_test_length_1], Y_test, verbose=ver)\n",
    "                Y_pred = model.predict([X_test_1,X_test_attribute_1,X_test_length_1])\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\n",
    "            elif mod == 'M2':\n",
    "                scores = model.evaluate(X_test, Y_test, verbose=ver)\n",
    "                Y_pred = model.predict(X_test)\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\n",
    "            elif mod == 'M3':\n",
    "                scores = model.evaluate(X_test_3, Y_test_3, verbose=ver)\n",
    "                Y_pred = model.predict(X_test_3)\n",
    "                Y_pred_disc = np.round(Y_pred)\n",
    "                Y_pred_disc = Y_pred_disc.reshape((Y_pred_disc.shape[1]*Y_pred_disc.shape[0],1))\n",
    "                Y_test_disc = Y_test_3.reshape((Y_test_3.shape[1]*Y_test_3.shape[0],1))\n",
    "                print(Y_test_disc.shape,Y_pred_disc.shape)\n",
    "\n",
    "            # Stop the stopwatch / counter\n",
    "            t3_stop = time.process_time()\n",
    "            t3_time = t3_stop-t3_start\n",
    "\n",
    "            print('-----------------------------------------------------------------------------')\n",
    "            Summe = test.groupby('value')['value_x'].count()\n",
    "            #print('Error Rate: '+ str(round(100/(Summe[0]+Summe[1])*Summe[1],2)))\n",
    "            loss = scores[0]\n",
    "            print('Loss: {:.2f}'.format(loss))\n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = accuracy_score(Y_test_disc, Y_pred_disc)\n",
    "            print('Accuracy: {:.2f}'.format(accuracy))\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = precision_score(Y_test_disc, Y_pred_disc)\n",
    "            print('Precision: {:.2f}'.format(precision))\n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = recall_score(Y_test_disc, Y_pred_disc)\n",
    "            print('Recall: {:.2f}'.format(recall))\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = f1_score(Y_test_disc, Y_pred_disc)\n",
    "            print('F1 score: {:.2f}'.format(f1))\n",
    "            print()\n",
    "            print('Traintime in sec: ',round(t1_time+t2_time,0))\n",
    "            print('Testtime in sec: ',round(t3_time,0))\n",
    "            print('Totaltime in sec: ',round(t1_time+t2_time+t3_time,0))\n",
    "\n",
    "            acc.append(round(accuracy,4))\n",
    "            pre.append(round(precision,4))\n",
    "            rec.append(round(recall,4))\n",
    "            F1.append(round(f1,4))\n",
    "\n",
    "            train_time.append(int(t1_time+t2_time))\n",
    "            test_time.append(int(t3_time))\n",
    "            t_time.append(int(t1_time+t2_time+t3_time))\n",
    "\n",
    "            if val == True:\n",
    "                if Iteration == 1:\n",
    "                    test_loss = log_test_loss\n",
    "                    train_loss = log_train_loss\n",
    "                    test_accuracy = log_test_accuracy\n",
    "                    train_accuracy = log_train_accuracy\n",
    "                else:\n",
    "                    test_loss = np.column_stack([test_loss,log_test_loss])\n",
    "                    train_loss = np.column_stack([train_loss,log_train_loss])\n",
    "                    test_accuracy = np.column_stack([test_accuracy,log_test_accuracy])\n",
    "                    train_accuracy = np.column_stack([train_accuracy,log_train_accuracy])\n",
    "\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_loss' + str(n) + '.csv', test_loss, delimiter=',')\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_loss' + str(n) +'.csv', train_loss, delimiter=',')\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_accuracy' + str(n) + '.csv', test_accuracy, delimiter=',')\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_accuracy' + str(n) +'.csv', train_accuracy, delimiter=',')\n",
    "\n",
    "        if run == 1:            \n",
    "            acc_pd = pd.DataFrame(acc,columns=[n])\n",
    "            pre_pd = pd.DataFrame(pre,columns=[n])\n",
    "            rec_pd = pd.DataFrame(rec,columns=[n])\n",
    "            F1_pd = pd.DataFrame(F1,columns=[n])\n",
    "\n",
    "            train_time_pd = pd.DataFrame(train_time,columns=[n])\n",
    "            test_time_pd = pd.DataFrame(test_time,columns=[n])\n",
    "            t_time_pd = pd.DataFrame(t_time,columns=[n])\n",
    "        else:            \n",
    "            acc_pd[n] = acc\n",
    "            pre_pd[n] = pre\n",
    "            rec_pd[n] = rec\n",
    "            F1_pd[n] = F1\n",
    "\n",
    "            train_time_pd[n] = train_time\n",
    "            test_time_pd[n] = test_time\n",
    "            t_time_pd[n] = t_time\n",
    "\n",
    "        try:\n",
    "            os.makedirs('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results')\n",
    "        except FileExistsError:\n",
    "            # directory already exists\n",
    "            pass\n",
    "        \n",
    "        if val == True:\n",
    "            print('No measures saved!')\n",
    "        else:\n",
    "            acc_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_acc.csv', index=False)\n",
    "            pre_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_pre.csv', index=False)\n",
    "            rec_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_rec.csv', index=False)\n",
    "            F1_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_F1.csv', index=False)\n",
    "\n",
    "            train_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_time.csv', index=False)\n",
    "            test_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_time.csv', index=False)\n",
    "            t_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_t_time.csv', index=False)\n",
    "            \n",
    "        print('-----------------------------------------------------------------------------')\n",
    "        print('Average scores for ' + mod)\n",
    "        print(f'> Accuracy: {round(np.mean(acc),2)} (+- {round(np.std(acc),2)})')\n",
    "        print(f'> Precison: {round(np.mean(pre),2)} (+- {round(np.std(pre),2)})')\n",
    "        print(f'> Recall: {round(np.mean(rec),2)} (+- {round(np.std(rec),2)})')\n",
    "        print(f'> F1: {round(np.mean(F1),2)} (+- {round(np.std(F1),2)})')\n",
    "        print(f'> Traintime in sec: {round(np.mean(train_time),0)} (+- {round(np.std(train_time),0)})')\n",
    "        print(f'> Testtime in sec: {round(np.mean(test_time),0)} (+- {round(np.std(test_time,0))})')\n",
    "        print(f'> Totaltime in sec: {round(np.mean(t_time),0)} (+- {round(np.std(t_time),0)})')\n",
    "        print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lists are not identical\n",
      "The dirty and clean have the same structure. We use the columnames from clean for dirty.\n",
      "Maximum value_x length:  52\n",
      "value\n",
      "0    22148\n",
      "1     4362\n",
      "Name: id_, dtype: int64\n",
      "\n",
      "Error Rate:0.16\n",
      "Number of characters: 86\n",
      "{' ': 1, 'e': 2, 'n': 3, 'r': 4, 'a': 5, '0': 6, 'o': 7, 'i': 8, '1': 9, 'A': 10, 'l': 11, '.': 12, '2': 13, 't': 14, 'm': 15, 'B': 16, '5': 17, '6': 18, 's': 19, 'c': 20, 'g': 21, 'C': 22, '9': 23, 'y': 24, '4': 25, 'u': 26, '3': 27, 'w': 28, 'P': 29, 'p': 30, 'd': 31, '7': 32, '8': 33, 'h': 34, 'z': 35, 'I': 36, 'S': 37, 'N': 38, '/': 39, 'k': 40, 'M': 41, 'b': 42, 'O': 43, 'R': 44, 'W': 45, 'L': 46, 'T': 47, 'H': 48, 'D': 49, 'v': 50, '%': 51, 'F': 52, 'G': 53, 'f': 54, '(': 55, ')': 56, 'E': 57, 'V': 58, 'K': 59, \"'\": 60, 'Z': 61, 'x': 62, '-': 63, 'U': 64, 'Y': 65, 'X': 66, 'J': 67, 'ö': 68, '&': 69, 'q': 70, 'j': 71, 'ä': 72, 'Q': 73, '’': 74, '#': 75, '!': 76, 'è': 77, ':': 78, ',': 79, '°': 80, 'é': 81, '™': 82, 'í': 83, 'ü': 84, '‘': 85, '?': 86}\n",
      "Number of attributs: 11\n",
      "{'index': 1, 'id': 2, 'beer-name': 3, 'style': 4, 'ounces': 5, 'abv': 6, 'ibu': 7, 'brewery_id': 8, 'brewery-name': 9, 'city': 10, 'state': 11}\n",
      "index\n",
      "Max lenght: 4 --> 4\n",
      "Unique characters: 10\n",
      "Number of errors: 0\n",
      "\n",
      "id\n",
      "Max lenght: 4 --> 4\n",
      "Unique characters: 10\n",
      "Number of errors: 0\n",
      "\n",
      "beer-name\n",
      "Max lenght: 52 --> 52\n",
      "Unique characters: 85\n",
      "Number of errors: 0\n",
      "\n",
      "style\n",
      "Max lenght: 35 --> 35\n",
      "Unique characters: 54\n",
      "Number of errors: 0\n",
      "\n",
      "ounces\n",
      "Max lenght: 18 --> 18\n",
      "Unique characters: 28\n",
      "Number of errors: 2410\n",
      "\n",
      "abv\n",
      "Max lenght: 21 --> 21\n",
      "Unique characters: 12\n",
      "Number of errors: 693\n",
      "\n",
      "ibu\n",
      "Max lenght: 3 --> 3\n",
      "Unique characters: 13\n",
      "Number of errors: 1005\n",
      "\n",
      "brewery_id\n",
      "Max lenght: 3 --> 3\n",
      "Unique characters: 10\n",
      "Number of errors: 0\n",
      "\n",
      "brewery-name\n",
      "Max lenght: 35 --> 35\n",
      "Unique characters: 66\n",
      "Number of errors: 0\n",
      "\n",
      "city\n",
      "Max lenght: 21 --> 21\n",
      "Unique characters: 54\n",
      "Number of errors: 127\n",
      "\n",
      "state\n",
      "Max lenght: 2 --> 2\n",
      "Unique characters: 24\n",
      "Number of errors: 127\n",
      "\n",
      "5/11 faulty attributes\n",
      "Time in sec:  0.2717579999999993\n"
     ]
    }
   ],
   "source": [
    "#Data Preparation\n",
    "\n",
    "# Start the stopwatch / counter \n",
    "t1_start = time.process_time() \n",
    "\n",
    "data = 'beers'\n",
    "dirty_table, clean_table = Input(data)\n",
    "dirty_table, clean_table, attribute, maxlen = Structure(True,dirty_table,clean_table)\n",
    "df, X_roh, y = Merge(dirty_table, clean_table)\n",
    "tk_char, tk_attr = Dictionary(attribute,df)\n",
    "\n",
    "#Extend Attribute\n",
    "attribute, X, y, Drop_list = attribute_extend(attribute,df,X_roh,y)\n",
    "\n",
    "# Stop the stopwatch / counter\n",
    "t1_stop = time.process_time()\n",
    "t1_time = t1_stop-t1_start\n",
    "\n",
    "print('Time in sec: ',t1_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for models\n",
    "n_classes = 2\n",
    "ver=0\n",
    "\n",
    "# Hyperparameter\n",
    "n_epochs = 120\n",
    "#batch_size=round((attribute.shape[0]-len(Drop_list))*n/4)\n",
    "batch_size=round((attribute.shape[0]-len(Drop_list))*5)\n",
    "batch_size_3=round(X.shape[0])\n",
    "\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.005, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.002, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "\n",
    "emb_dim_char = round(len(tk_char.word_index)+1)\n",
    "emb_dim_attr = round(len(tk_attr.word_index)+1)\n",
    "rnn_dim = 64\n",
    "rnn_dim_att = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 1/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188.  32.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 10:26:00.406900: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.05\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.93\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  754.0\n",
      "Testtime in sec:  14.0\n",
      "Totaltime in sec:  769.0\n",
      "Test: 2/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[188.  32.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.06\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.94\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  750.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  765.0\n",
      "Test: 3/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[189.  31.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 0.98\n",
      "Recall: 0.94\n",
      "F1 score: 0.96\n",
      "\n",
      "Traintime in sec:  746.0\n",
      "Testtime in sec:  14.0\n",
      "Totaltime in sec:  760.0\n",
      "Test: 4/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.05\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.94\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  761.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  776.0\n",
      "Test: 5/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[191.  29.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.05\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.94\n",
      "F1 score: 0.96\n",
      "\n",
      "Traintime in sec:  769.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  784.0\n",
      "Test: 6/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 0.96\n",
      "Recall: 0.94\n",
      "F1 score: 0.95\n",
      "\n",
      "Traintime in sec:  770.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  785.0\n",
      "Test: 7/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 0.98\n",
      "Recall: 0.94\n",
      "F1 score: 0.96\n",
      "\n",
      "Traintime in sec:  756.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  771.0\n",
      "Test: 8/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.03\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.95\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  760.0\n",
      "Testtime in sec:  14.0\n",
      "Totaltime in sec:  774.0\n",
      "Test: 9/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[193.  27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.06\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Recall: 0.94\n",
      "F1 score: 0.95\n",
      "\n",
      "Traintime in sec:  768.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  783.0\n",
      "Test: 10/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[190.  30.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.06\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.94\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  757.0\n",
      "Testtime in sec:  15.0\n",
      "Totaltime in sec:  772.0\n",
      "No measures saved!\n",
      "-----------------------------------------------------------------------------\n",
      "Average scores for M0\n",
      "> Accuracy: 0.99 (+- 0.0)\n",
      "> Precison: 0.99 (+- 0.01)\n",
      "> Recall: 0.94 (+- 0.0)\n",
      "> F1: 0.96 (+- 0.01)\n",
      "> Traintime in sec: 759.0 (+- 8.0)\n",
      "> Testtime in sec: 14.0 (+- 0)\n",
      "> Totaltime in sec: 773.0 (+- 8.0)\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Test(10,20,'M0','DiverSet',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 1/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[188.  32.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n",
      "F1 score: 0.98\n",
      "\n",
      "Traintime in sec:  798.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  814.0\n",
      "Test: 2/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[188.  32.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.06\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.97\n",
      "F1 score: 0.98\n",
      "\n",
      "Traintime in sec:  810.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  827.0\n",
      "Test: 3/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[189.  31.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.18\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.94\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  811.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  827.0\n",
      "Test: 4/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.13\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.94\n",
      "F1 score: 0.97\n",
      "\n",
      "Traintime in sec:  799.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  815.0\n",
      "Test: 5/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[191.  29.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.10\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.97\n",
      "F1 score: 0.98\n",
      "\n",
      "Traintime in sec:  804.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  820.0\n",
      "Test: 6/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.08\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n",
      "F1 score: 0.99\n",
      "\n",
      "Traintime in sec:  806.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  823.0\n",
      "Test: 7/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.06\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n",
      "F1 score: 0.99\n",
      "\n",
      "Traintime in sec:  795.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  810.0\n",
      "Test: 8/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[192.  28.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.97\n",
      "F1 score: 0.98\n",
      "\n",
      "Traintime in sec:  796.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  812.0\n",
      "Test: 9/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[193.  27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.11\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.93\n",
      "F1 score: 0.96\n",
      "\n",
      "Traintime in sec:  785.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  801.0\n",
      "Test: 10/10\n",
      "Number of train-tupels: 20 Sample technique: DiverSet\n",
      "[190.  30.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/d016vtvx3v5_j1m5f6s2dpl80000gn/T/ipykernel_18266/1314690276.py:112: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_Manuel_new = train_Manuel.append(train_Manuel_zus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Loss: 0.07\n",
      "Accuracy: 0.99\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n",
      "F1 score: 0.98\n",
      "\n",
      "Traintime in sec:  790.0\n",
      "Testtime in sec:  16.0\n",
      "Totaltime in sec:  806.0\n",
      "No measures saved!\n",
      "-----------------------------------------------------------------------------\n",
      "Average scores for M1\n",
      "> Accuracy: 0.99 (+- 0.0)\n",
      "> Precison: 1.0 (+- 0.01)\n",
      "> Recall: 0.96 (+- 0.02)\n",
      "> F1: 0.98 (+- 0.01)\n",
      "> Traintime in sec: 799.0 (+- 8.0)\n",
      "> Testtime in sec: 16.0 (+- 0)\n",
      "> Totaltime in sec: 815.0 (+- 8.0)\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Test(10,20,'M1','DiverSet',True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3074397aa67324493a4e19a94092cddfd49ffb744bae2a6319f30208afef7c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
