{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\r\n",
    "\r\n",
    "# Seed value\r\n",
    "seed_value= 0\r\n",
    "\r\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\r\n",
    "import os\r\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
    "\r\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\r\n",
    "import random\r\n",
    "random.seed(seed_value)\r\n",
    "\r\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\r\n",
    "import numpy as np\r\n",
    "np.random.seed(seed_value)\r\n",
    "\r\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\r\n",
    "import tensorflow as tf\r\n",
    "tf.random.set_seed(seed_value)\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "\r\n",
    "del seed_value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Input: We load both datasets (dirty and clean) as dirty_table and clean_table."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Input(data):\r\n",
    "    # Load Data\r\n",
    "    dirty_table = pd.read_csv('./datasets/' + data + '/dirty.csv', sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str, keep_default_na=False, low_memory=False)\r\n",
    "    clean_table = pd.read_csv('./datasets/' + data + '/clean.csv', sep=\",\", header=\"infer\", encoding=\"utf-8\", dtype=str, keep_default_na=False, low_memory=False)\r\n",
    "\r\n",
    "    return dirty_table, clean_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) Structure Transformation: Next we rename the column names in the dirty_table to have identical names with the clean dataset. We need this to combine the information of both datasets and create a new one (df). Also we add tid as sequence number for every row. At the end we cute the strings after 100 characters (numcharmax)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Structure(Tablestructure_equal, dirty_table, clean_table):\r\n",
    "    # Structure dirty_table and clean_table equal? (names of columns can be different)\r\n",
    "    #Tablestructure_equal = True\r\n",
    "\r\n",
    "    # Rename the different columnames\r\n",
    "    cols_clean_table = list(clean_table.columns.values)\r\n",
    "    cols_dirty_table = list(dirty_table.columns.values)\r\n",
    "\r\n",
    "    if sorted(cols_clean_table) == sorted(cols_dirty_table): \r\n",
    "        print (\"The lists are identical\")\r\n",
    "    else : \r\n",
    "        print (\"The lists are not identical\")\r\n",
    "        if Tablestructure_equal == True:\r\n",
    "            print (\"The dirty and clean have the same structure. We use the columnames from clean for dirty.\") \r\n",
    "            dirty_table.columns = cols_clean_table\r\n",
    "\r\n",
    "    # Add id_\r\n",
    "    clean_table.insert(0, 'id_', clean_table.index)\r\n",
    "    clean_table = clean_table.set_index('id_')\r\n",
    "\r\n",
    "    dirty_table.insert(0, 'id_', dirty_table.index)\r\n",
    "    dirty_table = dirty_table.set_index('id_')\r\n",
    "\r\n",
    "    dirty_table = dirty_table.replace(r'^\\s*$', np.nan, regex=True)\r\n",
    "    dirty_table = dirty_table.fillna('')\r\n",
    "    clean_table = clean_table.replace(r'^\\s*$', np.nan, regex=True)\r\n",
    "    clean_table = clean_table.fillna('')\r\n",
    "\r\n",
    "    # Generate table attribute with information about columns\r\n",
    "    attribute = pd.DataFrame(clean_table.columns.to_numpy(), columns = ['name'])\r\n",
    "    measurer = np.vectorize(len)\r\n",
    "    attribute['maxnumchar1'] = measurer(dirty_table.astype(str)).max(axis=0)\r\n",
    "    attribute['maxnumchar']=np.where(attribute['maxnumchar1']>128, 128, attribute['maxnumchar1'])\r\n",
    "\r\n",
    "    maxlen = np.max(attribute['maxnumchar'])\r\n",
    "    print(\"Maximum value_x length: \", maxlen)\r\n",
    "\r\n",
    "    return dirty_table, clean_table, attribute, maxlen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Merge: Next we combine the two tables in the dataset df where every cell of the dirty_table / clean_table is saved in the columns value_x / value_y, respectively. For the models we need an attribute value, i.e. a label, which includes 0 (correct) or 1 (wrong). We get this value when comparing value_x and value_y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Merge(dirty_table, clean_table):\r\n",
    "    # Produce datasets which transformed the table in rows\r\n",
    "    clean_row=clean_table.unstack().reset_index()\r\n",
    "    clean_row['Sort'] = clean_row.index\r\n",
    "    clean_row = clean_row.rename(columns={'level_0':'attribute','level_1':'id_',0:'value'}).sort_values(by=['id_','Sort'])\r\n",
    "    clean_row=clean_row.reset_index(drop=True).drop(columns='Sort')\r\n",
    "\r\n",
    "    dirty_row=dirty_table.unstack().reset_index()\r\n",
    "    dirty_row['Sort'] = dirty_row.index\r\n",
    "    dirty_row = dirty_row.rename(columns={'level_0':'attribute','level_1':'id_',0:'value'}).sort_values(by=['id_','Sort'])\r\n",
    "    dirty_row=dirty_row.reset_index(drop=True).drop(columns='Sort')\r\n",
    "\r\n",
    "    # Produce datasets for M2\r\n",
    "    X_roh = dirty_table\r\n",
    "    y = clean_table != dirty_table\r\n",
    "\r\n",
    "    y = y.astype(int)\r\n",
    "\r\n",
    "    # Merge datasets together\r\n",
    "    df = pd.merge(dirty_row, clean_row, on=['id_', \"attribute\"])\r\n",
    "\r\n",
    "    # Show rows which are empty (1)\r\n",
    "    df['empty1'] = np.where(np.isin(df['value_x'].str.lower(),['', 'nan','n/a','n/n']) == True,1,0)\r\n",
    "\r\n",
    "    # Compare content of dirty and clean dataset\r\n",
    "    df['value'] = np.where(df['value_x'] == df['value_y'], 0, 1)\r\n",
    "\r\n",
    "    # Concatenate attributename and value_x (dirty)\r\n",
    "    df['concat'] = df['attribute'] + '_' + df['value_x']\r\n",
    "\r\n",
    "    df['length'] = df.value_x.str.len()\r\n",
    "\r\n",
    "    Summe = df.groupby('value')['id_'].count()\r\n",
    "    print(Summe)\r\n",
    "    print()\r\n",
    "    print('Error Rate:'+ str(round(1/(Summe[0]+Summe[1])*Summe[1],2)))\r\n",
    "\r\n",
    "    return df, X_roh, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Dictionary Generation: Before we can feed the data into a neural network, we need to transform the data types from character to numeric character embedding. We produce a value dictionary (char_index) which contains an index for each character in value_x.\r\n",
    "\r\n",
    "For the ETSB-RNN we also need an attribute dictionary (attribute_index) which includes an index for each attribute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Dictionary(attribute,df):\r\n",
    "    # Tokenizer character\r\n",
    "    tk_char = tf.keras.preprocessing.text.Tokenizer(num_words=False, lower=False, char_level=True)\r\n",
    "    tk_char.fit_on_texts(df.value_x)\r\n",
    "    tk_char_len=len(tk_char.word_index)\r\n",
    "    print(\"Number of characters: \" + str(tk_char_len))\r\n",
    "    print(tk_char.word_index)\r\n",
    "    #tk_char_list = list(tk_char.word_index.keys())\r\n",
    "\r\n",
    "    # Tokenizer attribute\r\n",
    "    tk_attr = tf.keras.preprocessing.text.Tokenizer(num_words=False, filters='', lower=False, char_level=False, split=\"nosplit\")\r\n",
    "    tk_attr.fit_on_texts(df.attribute)\r\n",
    "    print(\"Number of attributs: \" + str(len(tk_attr.word_index)))\r\n",
    "    print(tk_attr.word_index)\r\n",
    "\r\n",
    "    return tk_char, tk_attr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def attribute_extend(attribute,df,X_roh,y):\r\n",
    "    # Print properties (length and number of errors per column)\r\n",
    "    i=0\r\n",
    "    num_error_col=0\r\n",
    "    Drop_list = []\r\n",
    "    for attr in attribute['name']:\r\n",
    "        df2 = df[df['attribute']==attr]\r\n",
    "        maxnumchar = attribute.loc[attribute['name']==attr]['maxnumchar'].to_numpy()[0]\r\n",
    "        maxnumchar1 = attribute.loc[attribute['name']==attr]['maxnumchar1'].to_numpy()[0]\r\n",
    "        summe = np.sum(df2.value)\r\n",
    "        attribute.loc[attribute['name'] == attr, 'error'] = int(summe)\r\n",
    "        \r\n",
    "        #if maxnumchar1 > 500:\r\n",
    "        if maxnumchar1 > 10000:\r\n",
    "            Drop_list.append(attr)\r\n",
    "        else:\r\n",
    "            i=i+1\r\n",
    "            arr = tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(X_roh[attr].astype(str)), maxlen=maxnumchar, padding='post')\r\n",
    "            if i == 1:\r\n",
    "                X = arr\r\n",
    "            else:\r\n",
    "                X = np.concatenate((X, arr), axis=1)\r\n",
    "                \r\n",
    "        tk_perCol = tf.keras.preprocessing.text.Tokenizer(num_words=False, lower=False, char_level=True)\r\n",
    "        tk_perCol.fit_on_texts(X_roh[attr])\r\n",
    "        attribute.loc[attribute['name'] == attr, 'numuniquechar'] = len(tk_perCol.word_index)\r\n",
    "        df.loc[df['attribute'] == attr, 'length_norm'] = df['length']/maxnumchar1\r\n",
    "\r\n",
    "        print(attr)\r\n",
    "        print('Max lenght: ' + str(maxnumchar1) + ' --> ' + str(maxnumchar))\r\n",
    "        print('Unique characters: ' + str(len(tk_perCol.word_index)))\r\n",
    "        #print(tk_perCol.word_index)\r\n",
    "        print('Number of errors: ' + str(summe))\r\n",
    "        print('')\r\n",
    "        if summe > 0:\r\n",
    "            num_error_col+=1\r\n",
    "\r\n",
    "    print(str(num_error_col) + '/' + str(len(attribute)) + ' faulty attributes')\r\n",
    "\r\n",
    "    X = pd.DataFrame(X)\r\n",
    "    X.insert(0, 'id_', X.index)\r\n",
    "    X = X.set_index('id_')\r\n",
    "    y = pd.DataFrame(y.drop(columns=Drop_list))\r\n",
    "    X.reset_index(level=0, inplace=True)\r\n",
    "    y.reset_index(level=0, inplace=True)\r\n",
    "\r\n",
    "    return attribute, X, y, Drop_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chose_Model(Mod):\r\n",
    "    global model\r\n",
    "    global checkpoint_path\r\n",
    "    global checkpoint\r\n",
    "\r\n",
    "    tf.keras.backend.clear_session()\r\n",
    "\r\n",
    "    checkpoint_path = 'checkpoint/' + data + '/p12/checkpoint_p12_' + Mod\r\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n",
    "        filepath=checkpoint_path,\r\n",
    "        monitor='loss',\r\n",
    "        save_best_only=True,\r\n",
    "        save_weights_only=True,\r\n",
    "        verbose=ver\r\n",
    "    )\r\n",
    "    \r\n",
    "    if Mod=='M0':\r\n",
    "        # Define TSB-RNN\r\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\r\n",
    "\r\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\r\n",
    "\r\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\r\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(x)\r\n",
    "        x = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(x)\r\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\r\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(x)\r\n",
    "\r\n",
    "        model = tf.keras.models.Model(inputs=inputA, outputs=z)\r\n",
    "\r\n",
    "    elif Mod=='M1alt':\r\n",
    "        # Define ETSB-RNN\r\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\r\n",
    "        inputB = tf.keras.Input(shape=(1,))\r\n",
    "\r\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\r\n",
    "        b = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(inputB)\r\n",
    "\r\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\r\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(x)\r\n",
    "        x = tf.keras.models.Model(inputs=inputA, outputs=x)\r\n",
    "\r\n",
    "        y = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=True))(b)\r\n",
    "        y = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=False))(y)\r\n",
    "        y = tf.keras.models.Model(inputs=inputB, outputs=y)\r\n",
    "\r\n",
    "        combined = tf.keras.layers.concatenate([x.output, y.output])\r\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(combined)\r\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\r\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\r\n",
    "\r\n",
    "        model = tf.keras.models.Model(inputs=[x.input, y.input], outputs=z)\r\n",
    "\r\n",
    "    elif Mod=='M1':\r\n",
    "        # Define ETSB-RNN new\r\n",
    "        inputA = tf.keras.Input(shape=(maxlen,))\r\n",
    "        a = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(inputA)\r\n",
    "        a = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=True))(a)\r\n",
    "        a = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim, return_sequences=False))(a)\r\n",
    "\r\n",
    "        inputB = tf.keras.Input(shape=(1,))\r\n",
    "        b = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(inputB)\r\n",
    "        b = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=True))(b)\r\n",
    "        b = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=rnn_dim_att, return_sequences=False))(b)\r\n",
    "\r\n",
    "        inputC = tf.keras.Input(shape=(1,))\r\n",
    "        c = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(inputC)\r\n",
    "        c = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(c)\r\n",
    "\r\n",
    "        combined = tf.keras.layers.concatenate([a, b, c])\r\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim/2), activation=\"relu\")(combined)\r\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\r\n",
    "\r\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\r\n",
    "\r\n",
    "        model = tf.keras.models.Model(inputs=[inputA, inputB, inputC], outputs=z)\r\n",
    "\r\n",
    "    elif Mod=='M2':\r\n",
    "        # Define MTSB-RNN\r\n",
    "        crop1=0\r\n",
    "        width_1=X.shape[1]+maxlen\r\n",
    "        crop2=width_1\r\n",
    "\r\n",
    "        inputs = tf.keras.Input(shape=(width_1,))\r\n",
    "        inputsIn = tf.keras.layers.Reshape((width_1,1))(inputs)\r\n",
    "        #inputsIn = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char)(inputs)\r\n",
    "        all_inputs = []\r\n",
    "        all_outputs = []\r\n",
    "\r\n",
    "        for index, row in attribute.iterrows():\r\n",
    "            numuniquechar=int(row['numuniquechar']+1)\r\n",
    "            maxnumchar=row['maxnumchar']\r\n",
    "            In = 'In'+str(index)\r\n",
    "            crop2-=maxnumchar\r\n",
    "\r\n",
    "            In = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\r\n",
    "            In = tf.keras.layers.Reshape((maxnumchar,), input_shape=(maxnumchar,1))(In)\r\n",
    "            In = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In)\r\n",
    "            In = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=True))(In)\r\n",
    "            In = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=False))(In)\r\n",
    "            all_inputs.append(In)\r\n",
    "\r\n",
    "            crop1+=maxnumchar\r\n",
    "\r\n",
    "        crop2-=maxlen\r\n",
    "        In_value = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\r\n",
    "        In_value = tf.keras.layers.Reshape((maxlen,), input_shape=(maxlen,1))(In_value)\r\n",
    "        In_value = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In_value)\r\n",
    "        In_value = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=True))(In_value)\r\n",
    "        In_value = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim)), return_sequences=False))(In_value)\r\n",
    "        all_inputs.append(In_value)\r\n",
    "        crop1+=maxlen\r\n",
    "\r\n",
    "        crop2-=1\r\n",
    "        In_attr = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\r\n",
    "        In_attr = tf.keras.layers.Reshape((1,), input_shape=(1,1))(In_attr)\r\n",
    "        In_attr = tf.keras.layers.Embedding(emb_dim_attr,emb_dim_attr)(In_attr)\r\n",
    "        In_attr = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim_att)), return_sequences=True))(In_attr)\r\n",
    "        In_attr = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((rnn_dim_att)), return_sequences=False))(In_attr)\r\n",
    "        all_inputs.append(In_attr)\r\n",
    "        crop1+=1\r\n",
    "\r\n",
    "        combined = tf.keras.layers.concatenate(all_inputs)\r\n",
    "        combined = tf.keras.layers.Dense(round(rnn_dim), activation=\"relu\")(combined)\r\n",
    "        combined = tf.keras.layers.BatchNormalization()(combined)\r\n",
    "\r\n",
    "        z = tf.keras.layers.Dense(n_classes, activation='softmax')(combined)\r\n",
    "\r\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=z)\r\n",
    "\r\n",
    "    elif Mod=='M3':\r\n",
    "        crop1=0\r\n",
    "        width_1=X.shape[1]-1\r\n",
    "        crop2=width_1\r\n",
    "        \r\n",
    "        inputs = tf.keras.Input(shape=(width_1,))\r\n",
    "        inputsIn = tf.keras.layers.Reshape((width_1,1))(inputs)\r\n",
    "        all_inputs1 = []\r\n",
    "        all_inputs2 = []\r\n",
    "        all_outputs = []\r\n",
    "\r\n",
    "        for index, row in attribute.iterrows():\r\n",
    "            numuniquechar=int(row['numuniquechar']+1)\r\n",
    "            maxnumchar=row['maxnumchar']\r\n",
    "            In = 'In'+str(index)\r\n",
    "            crop2-=maxnumchar\r\n",
    "            \r\n",
    "            In = tf.keras.layers.Cropping1D(cropping=(crop1,crop2))(inputsIn)\r\n",
    "            In = tf.keras.layers.Reshape((maxnumchar,), input_shape=(maxnumchar,1))(In)\r\n",
    "            In = tf.keras.layers.Embedding(emb_dim_char,emb_dim_char,mask_zero=True)(In)\r\n",
    "\r\n",
    "            In1 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round(numuniquechar/2), return_sequences=True))(In)\r\n",
    "            In1 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round(numuniquechar/2)))(In1)\r\n",
    "            In1 = tf.keras.layers.Dense(numuniquechar, activation=\"relu\")(In1)\r\n",
    "            In1 = tf.keras.layers.Dropout(0.1)(In1)\r\n",
    "\r\n",
    "            In2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((emb_dim_char)/2), return_sequences=True))(In)\r\n",
    "            In2 = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(units=round((emb_dim_char)/2)))(In2)\r\n",
    "            In2 = tf.keras.layers.Dense(emb_dim_char, activation=\"relu\")(In2)\r\n",
    "            In2 = tf.keras.layers.Dropout(0.1)(In2)\r\n",
    "\r\n",
    "            all_inputs1.append(In1)\r\n",
    "            all_inputs2.append(In2)\r\n",
    "\r\n",
    "            crop1+=maxnumchar\r\n",
    "\r\n",
    "        combined1 = tf.keras.layers.concatenate(all_inputs1)\r\n",
    "        combined1 = tf.keras.layers.Dense(round(combined1.shape[1]/2), activation=\"relu\")(combined1)\r\n",
    "        combined1 = tf.keras.layers.Dropout(0.1)(combined1)\r\n",
    "\r\n",
    "        combined2 = tf.keras.layers.concatenate(all_inputs2)\r\n",
    "        combined2 = tf.keras.layers.Dense(emb_dim_char, activation=\"relu\")(combined2)\r\n",
    "        combined2 = tf.keras.layers.Dropout(0.1)(combined2)\r\n",
    "\r\n",
    "        combined = tf.keras.layers.concatenate([combined1, combined2])\r\n",
    "\r\n",
    "        for index, row in attribute.iterrows():\r\n",
    "            Out = 'Out'+str(index)\r\n",
    "            Out = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\r\n",
    "\r\n",
    "            all_outputs.append(Out)\r\n",
    "\r\n",
    "        combinedOut = tf.keras.layers.concatenate(all_outputs)\r\n",
    "\r\n",
    "        model = tf.keras.Model(inputs=inputs,outputs=combinedOut)\r\n",
    "\r\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
    "\r\n",
    "    #model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def raha(num_sample):\r\n",
    "    import pandas\r\n",
    "    import IPython.display\r\n",
    "\r\n",
    "    import raha\r\n",
    "    \r\n",
    "    app_1 = raha.Detection()\r\n",
    "    app_1.LABELING_BUDGET = num_sample\r\n",
    "\r\n",
    "    dataset_dictionary = {\r\n",
    "        \"name\": data,\r\n",
    "        \"path\": './datasets/' + data + '/dirty.csv',\r\n",
    "        \"clean_path\": './datasets/' + data + '/clean.csv'\r\n",
    "    }\r\n",
    "\r\n",
    "    d = app_1.initialize_dataset(dataset_dictionary)\r\n",
    "\r\n",
    "    app_1.run_strategies(d)\r\n",
    "\r\n",
    "    app_1.generate_features(d)\r\n",
    "\r\n",
    "    app_1.build_clusters(d)\r\n",
    "\r\n",
    "    sampled_list = []\r\n",
    "\r\n",
    "    while len(d.labeled_tuples) < app_1.LABELING_BUDGET:\r\n",
    "        app_1.sample_tuple(d)\r\n",
    "        if d.has_ground_truth:\r\n",
    "            app_1.label_with_ground_truth(d)\r\n",
    "\r\n",
    "        sampled_list.append(d.sampled_tuple)\r\n",
    "\r\n",
    "    return sampled_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Test(Iteration_n,end_n,mod,sample_technique,val):\r\n",
    "    ID_Alle = df.groupby(['id_'], as_index=False)['value'].sum()\r\n",
    "    run=0\r\n",
    "    for n in range(20,end_n+1,10):\r\n",
    "        run+=1\r\n",
    "\r\n",
    "        Loss = []\r\n",
    "        acc = []\r\n",
    "        pre = []\r\n",
    "        rec = []\r\n",
    "        F1 = []\r\n",
    "\r\n",
    "        train_time = []\r\n",
    "        test_time = []\r\n",
    "        t_time = []\r\n",
    "        \r\n",
    "        for Iteration in range(1,Iteration_n+1):\r\n",
    "            # Start the stopwatch / counter \r\n",
    "            t2_start = time.process_time()\r\n",
    "\r\n",
    "            print('Test: ' + str(Iteration) + '/' + str(Iteration_n))\r\n",
    "            train = df[df['id_'].isin(ID_Alle['id_'])]\r\n",
    "            train_ID = ID_Alle['id_']\r\n",
    "\r\n",
    "            train_ID_Rest = ID_Alle['id_']\r\n",
    "            train_Rest = df[df['id_'].isin(train_ID_Rest)]\r\n",
    "            train_Rest = train_Rest[~train_Rest.attribute.isin(Drop_list)]\r\n",
    "\r\n",
    "            if sample_technique == 'RahaSet':\r\n",
    "                train_ID_Manuel = pd.Series(raha(n))\r\n",
    "                train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\r\n",
    "                train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\r\n",
    "                train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\r\n",
    "                train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\r\n",
    "\r\n",
    "            elif sample_technique == 'RandomSet':\r\n",
    "                #train_ID_Manuel = pd.Series(ID_Alle.sample(n, random_state=Iteration)['id_'])\r\n",
    "                train_ID_Manuel = pd.Series(ID_Alle.sample(n)['id_'])\r\n",
    "                train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\r\n",
    "                train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\r\n",
    "                train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\r\n",
    "                train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\r\n",
    "\r\n",
    "            elif sample_technique == 'DiverSet':\r\n",
    "                train_ID_Manuel_List = []\r\n",
    "\r\n",
    "                # Iterate for choosing the next observation\r\n",
    "                for i in range(0,n):\r\n",
    "                    # For prefering empty value_x we have to compute the number of this\r\n",
    "                    empty = train_Rest.groupby(['id_'])['empty1'].agg('sum')\r\n",
    "                    count = train_Rest.groupby(['id_']).size().to_frame()\r\n",
    "                    count['empty1'] = empty\r\n",
    "                    count = count.sort_values(by=[0,'empty1'], ascending=False)\r\n",
    "                    count.reset_index(inplace=True)\r\n",
    "                    count = count[count[0]==count[0].max()]\r\n",
    "                    count = count[count['empty1']==count['empty1'].max()]\r\n",
    "                    train_ID_Manuel_List.append(count.sample(1, random_state=Iteration)['id_'])\r\n",
    "                    train_ID_Manuel = pd.Series(train_ID_Manuel_List)\r\n",
    "                    train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\r\n",
    "                    train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\r\n",
    "                    train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\r\n",
    "                    train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\r\n",
    "\r\n",
    "                del i, count, train_ID_Manuel_List, empty\r\n",
    "\r\n",
    "            elif sample_technique == 'sev1':\r\n",
    "                train_ID_Manuel_List = []\r\n",
    "\r\n",
    "                # Iterate for choosing the next observation\r\n",
    "                for i in range(0,n):\r\n",
    "                    # For prefering empty value_x we have to compute the number of this\r\n",
    "                    empty = train_Rest.groupby(['id_'])['empty1'].agg('sum')\r\n",
    "                    count = train_Rest.groupby(['id_']).size().to_frame()\r\n",
    "                    length = train_Rest.groupby(['id_'])['length_norm'].agg('sum')\r\n",
    "                    count['empty1'] = empty\r\n",
    "                    count['length1'] = length\r\n",
    "                    count = count.sort_values(by=[0,'empty1','length1'], ascending=False)\r\n",
    "                    count.reset_index(inplace=True)\r\n",
    "                    count = count[count[0]==count[0].min()]\r\n",
    "                    count = count[count['empty1']==count['empty1'].max()]\r\n",
    "                    #count = count[count['length1']==count['length1'].min()]\r\n",
    "                    train_ID_Manuel_List.append(count.sample(1, random_state=Iteration)['id_'])\r\n",
    "                    train_ID_Manuel = pd.Series(train_ID_Manuel_List)\r\n",
    "                    train_Manuel = df[df['id_'].isin(train_ID_Manuel)]\r\n",
    "                    train_Manuel = train_Manuel[~train_Manuel.attribute.isin(Drop_list)]\r\n",
    "                    train_Rest = train_Rest[~train_Rest.concat.isin(train_Manuel.concat)]\r\n",
    "                    train_ID_Rest = train_ID[~train_ID.isin(train_ID_Manuel)]\r\n",
    "\r\n",
    "                del i, count, train_ID_Manuel_List, empty\r\n",
    "\r\n",
    "            print('Number of train-tupels: ' + str(len(train_ID_Manuel)) + ' Sample technique: ' + str(sample_technique))\r\n",
    "\r\n",
    "            # The records which we dont need for training we use for the testing\r\n",
    "            test_ID = train_ID_Rest.copy()\r\n",
    "            #if data == 'Tax':\r\n",
    "            #    test_ID = test_ID.sample(10000)\r\n",
    "            #    print(data + ' is used! -> short version for testing')\r\n",
    "            #elif data == 'Movies':\r\n",
    "            #    test_ID = test_ID.sample(1500)\r\n",
    "            #    print(data + ' is used! -> short version for testing')\r\n",
    "            test = df[df['id_'].isin(test_ID)]\r\n",
    "            test = test[~test.attribute.isin(Drop_list)]\r\n",
    "\r\n",
    "            X_train_3 = np.array(X[X['id_'].isin(train_ID_Manuel)].drop(columns='id_'))\r\n",
    "            Y_train_3 = np.array(y[y['id_'].isin(train_ID_Manuel)].drop(columns='id_'))\r\n",
    "            X_test_3 = np.array(X[X['id_'].isin(test_ID)].drop(columns='id_'))\r\n",
    "            Y_test_3 = np.array(y[y['id_'].isin(test_ID)].drop(columns='id_'))\r\n",
    "\r\n",
    "            del train_ID, train, train_ID_Rest, train_Rest\r\n",
    "\r\n",
    "            train_Manuel_zus = pd.DataFrame()\r\n",
    "            train_Manuel_new = train_Manuel.append(train_Manuel_zus)\r\n",
    "\r\n",
    "            # Transform the text to numbers\r\n",
    "            X_train_Manuel=pd.DataFrame(tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(train_Manuel_new.value_x), maxlen=maxlen, padding='post'))\r\n",
    "            X_train_1=np.array(X_train_Manuel)\r\n",
    "            X_train_Manuel['id_']=np.array(train_Manuel_new['id_'])\r\n",
    "\r\n",
    "            X_train_Manuel_attribute=pd.DataFrame(tk_attr.texts_to_sequences(train_Manuel_new.attribute))\r\n",
    "            X_train_attribute_1=np.array(X_train_Manuel_attribute)\r\n",
    "            X_train_Manuel_attribute['id_']=np.array(train_Manuel_new['id_'])\r\n",
    "\r\n",
    "            X_train_length_1=train_Manuel_new.length_norm.to_numpy()\r\n",
    "\r\n",
    "            Y_train=tf.keras.utils.to_categorical(train_Manuel_new.value, num_classes=2)\r\n",
    "            print(np.ndarray.sum(Y_train,axis=0))\r\n",
    "\r\n",
    "            X_test_Manuel=pd.DataFrame(tf.keras.preprocessing.sequence.pad_sequences(tk_char.texts_to_sequences(test.value_x), maxlen=maxlen, padding='post'))\r\n",
    "            X_test_1=np.array(X_test_Manuel)\r\n",
    "            X_test_Manuel['id_']=np.array(test['id_'])\r\n",
    "\r\n",
    "            X_test_Manuel_attribute=pd.DataFrame(tk_attr.texts_to_sequences(test.attribute))\r\n",
    "            X_test_attribute_1=np.array(X_test_Manuel_attribute)\r\n",
    "            X_test_Manuel_attribute['id_']=np.array(test['id_'])\r\n",
    "\r\n",
    "            X_test_length_1=test.length_norm.to_numpy()\r\n",
    "\r\n",
    "            Y_test=tf.keras.utils.to_categorical(test.value, num_classes=2)\r\n",
    "\r\n",
    "            X_train=pd.merge(X, X_train_Manuel, on=['id_'])\r\n",
    "            X_train['attr']=X_train_Manuel_attribute[0]\r\n",
    "            X_train=np.array(X_train.drop(columns='id_'))\r\n",
    "\r\n",
    "            X_test=pd.merge(X, X_test_Manuel, on=['id_'])\r\n",
    "            X_test['attr']=X_test_Manuel_attribute[0]\r\n",
    "            X_test=np.array(X_test.drop(columns='id_'))\r\n",
    "            \r\n",
    "            chose_Model(mod)\r\n",
    "            # Train TSB-RNN (M0)\r\n",
    "            if mod == 'M0':\r\n",
    "                if val == True:\r\n",
    "                    log = model.fit(X_train_1, Y_train, validation_data=(X_test_1, Y_test), shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "                    log_test_loss = log.history['val_loss']\r\n",
    "                    log_train_loss = log.history['loss']\r\n",
    "                    log_test_accuracy = log.history['val_accuracy']\r\n",
    "                    log_train_accuracy = log.history['accuracy']\r\n",
    "                else:\r\n",
    "                    log = model.fit(X_train_1, Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "            # Train ETSB-RNN (M1)\r\n",
    "            elif mod == 'M1':\r\n",
    "                if val == True:\r\n",
    "                    log = model.fit(x=[X_train_1,X_train_attribute_1,X_train_length_1], y=Y_train, validation_data=([X_test_1,X_test_attribute_1,X_test_length_1], Y_test), shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "                    log_test_loss = log.history['val_loss']\r\n",
    "                    log_train_loss = log.history['loss']\r\n",
    "                    log_test_accuracy = log.history['val_accuracy']\r\n",
    "                    log_train_accuracy = log.history['accuracy']\r\n",
    "                else:\r\n",
    "                    log = model.fit(x=[X_train_1,X_train_attribute_1,X_train_length_1], y=Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "            elif mod == 'M2':\r\n",
    "                log = model.fit(x=X_train, y=Y_train, shuffle=True, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "            elif mod == 'M3':\r\n",
    "                log = model.fit(x=X_train_3, y=Y_train_3, shuffle=True, batch_size=batch_size_3, epochs=n_epochs, callbacks=[checkpoint], verbose=ver)\r\n",
    "            time.sleep(3)\r\n",
    "\r\n",
    "            # Stop the stopwatch / counter\r\n",
    "            t2_stop = time.process_time()\r\n",
    "            t2_time = t2_stop-t2_start\r\n",
    "\r\n",
    "            # Start the stopwatch / counter \r\n",
    "            t3_start = time.process_time()\r\n",
    "\r\n",
    "            # Load best weights\r\n",
    "            model.load_weights(checkpoint_path)\r\n",
    "\r\n",
    "            # Evaluate with testsets\r\n",
    "            if mod == 'M0':\r\n",
    "                scores = model.evaluate(X_test_1, Y_test, verbose=ver)\r\n",
    "                Y_pred = model.predict(X_test_1)\r\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\r\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\r\n",
    "            elif mod == 'M1':\r\n",
    "                scores = model.evaluate([X_test_1,X_test_attribute_1,X_test_length_1], Y_test, verbose=ver)\r\n",
    "                Y_pred = model.predict([X_test_1,X_test_attribute_1,X_test_length_1])\r\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\r\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\r\n",
    "            elif mod == 'M2':\r\n",
    "                scores = model.evaluate(X_test, Y_test, verbose=ver)\r\n",
    "                Y_pred = model.predict(X_test)\r\n",
    "                Y_pred_disc = np.argmax(Y_pred, axis=1)\r\n",
    "                Y_test_disc = np.argmax(Y_test, axis=1)\r\n",
    "            elif mod == 'M3':\r\n",
    "                scores = model.evaluate(X_test_3, Y_test_3, verbose=ver)\r\n",
    "                Y_pred = model.predict(X_test_3)\r\n",
    "                Y_pred_disc = np.round(Y_pred)\r\n",
    "                Y_pred_disc = Y_pred_disc.reshape((Y_pred_disc.shape[1]*Y_pred_disc.shape[0],1))\r\n",
    "                Y_test_disc = Y_test_3.reshape((Y_test_3.shape[1]*Y_test_3.shape[0],1))\r\n",
    "                print(Y_test_disc.shape,Y_pred_disc.shape)\r\n",
    "\r\n",
    "            # Stop the stopwatch / counter\r\n",
    "            t3_stop = time.process_time()\r\n",
    "            t3_time = t3_stop-t3_start\r\n",
    "\r\n",
    "            print('-----------------------------------------------------------------------------')\r\n",
    "            Summe = test.groupby('value')['value_x'].count()\r\n",
    "            #print('Error Rate: '+ str(round(100/(Summe[0]+Summe[1])*Summe[1],2)))\r\n",
    "            loss = scores[0]\r\n",
    "            print('Loss: {:.2f}'.format(loss))\r\n",
    "            # accuracy: (tp + tn) / (p + n)\r\n",
    "            accuracy = accuracy_score(Y_test_disc, Y_pred_disc)\r\n",
    "            print('Accuracy: {:.2f}'.format(accuracy))\r\n",
    "            # precision tp / (tp + fp)\r\n",
    "            precision = precision_score(Y_test_disc, Y_pred_disc)\r\n",
    "            print('Precision: {:.2f}'.format(precision))\r\n",
    "            # recall: tp / (tp + fn)\r\n",
    "            recall = recall_score(Y_test_disc, Y_pred_disc)\r\n",
    "            print('Recall: {:.2f}'.format(recall))\r\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\r\n",
    "            f1 = f1_score(Y_test_disc, Y_pred_disc)\r\n",
    "            print('F1 score: {:.2f}'.format(f1))\r\n",
    "            print()\r\n",
    "            print('Traintime in sec: ',round(t1_time+t2_time,0))\r\n",
    "            print('Testtime in sec: ',round(t3_time,0))\r\n",
    "            print('Totaltime in sec: ',round(t1_time+t2_time+t3_time,0))\r\n",
    "\r\n",
    "            acc.append(round(accuracy,4))\r\n",
    "            pre.append(round(precision,4))\r\n",
    "            rec.append(round(recall,4))\r\n",
    "            F1.append(round(f1,4))\r\n",
    "\r\n",
    "            train_time.append(int(t1_time+t2_time))\r\n",
    "            test_time.append(int(t3_time))\r\n",
    "            t_time.append(int(t1_time+t2_time+t3_time))\r\n",
    "\r\n",
    "            if val == True:\r\n",
    "                if Iteration == 1:\r\n",
    "                    test_loss = log_test_loss\r\n",
    "                    train_loss = log_train_loss\r\n",
    "                    test_accuracy = log_test_accuracy\r\n",
    "                    train_accuracy = log_train_accuracy\r\n",
    "                else:\r\n",
    "                    test_loss = np.column_stack([test_loss,log_test_loss])\r\n",
    "                    train_loss = np.column_stack([train_loss,log_train_loss])\r\n",
    "                    test_accuracy = np.column_stack([test_accuracy,log_test_accuracy])\r\n",
    "                    train_accuracy = np.column_stack([train_accuracy,log_train_accuracy])\r\n",
    "\r\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_loss' + str(n) + '.csv', test_loss, delimiter=',')\r\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_loss' + str(n) +'.csv', train_loss, delimiter=',')\r\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_accuracy' + str(n) + '.csv', test_accuracy, delimiter=',')\r\n",
    "                np.savetxt('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_accuracy' + str(n) +'.csv', train_accuracy, delimiter=',')\r\n",
    "\r\n",
    "        if run == 1:            \r\n",
    "            acc_pd = pd.DataFrame(acc,columns=[n])\r\n",
    "            pre_pd = pd.DataFrame(pre,columns=[n])\r\n",
    "            rec_pd = pd.DataFrame(rec,columns=[n])\r\n",
    "            F1_pd = pd.DataFrame(F1,columns=[n])\r\n",
    "\r\n",
    "            train_time_pd = pd.DataFrame(train_time,columns=[n])\r\n",
    "            test_time_pd = pd.DataFrame(test_time,columns=[n])\r\n",
    "            t_time_pd = pd.DataFrame(t_time,columns=[n])\r\n",
    "        else:            \r\n",
    "            acc_pd[n] = acc\r\n",
    "            pre_pd[n] = pre\r\n",
    "            rec_pd[n] = rec\r\n",
    "            F1_pd[n] = F1\r\n",
    "\r\n",
    "            train_time_pd[n] = train_time\r\n",
    "            test_time_pd[n] = test_time\r\n",
    "            t_time_pd[n] = t_time\r\n",
    "\r\n",
    "        try:\r\n",
    "            os.makedirs('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results')\r\n",
    "        except FileExistsError:\r\n",
    "            # directory already exists\r\n",
    "            pass\r\n",
    "        \r\n",
    "        if val == True:\r\n",
    "            print('No measures saved!')\r\n",
    "        else:\r\n",
    "            acc_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_acc.csv', index=False)\r\n",
    "            pre_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_pre.csv', index=False)\r\n",
    "            rec_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_rec.csv', index=False)\r\n",
    "            F1_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_F1.csv', index=False)\r\n",
    "\r\n",
    "            train_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_train_time.csv', index=False)\r\n",
    "            test_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_test_time.csv', index=False)\r\n",
    "            t_time_pd.to_csv('./datasets/' + data + '/' + mod + '_' + sample_technique + '_results/p12_t_time.csv', index=False)\r\n",
    "            \r\n",
    "        print('-----------------------------------------------------------------------------')\r\n",
    "        print('Average scores for ' + mod)\r\n",
    "        print(f'> Accuracy: {round(np.mean(acc),2)} (+- {round(np.std(acc),2)})')\r\n",
    "        print(f'> Precison: {round(np.mean(pre),2)} (+- {round(np.std(pre),2)})')\r\n",
    "        print(f'> Recall: {round(np.mean(rec),2)} (+- {round(np.std(rec),2)})')\r\n",
    "        print(f'> F1: {round(np.mean(F1),2)} (+- {round(np.std(F1),2)})')\r\n",
    "        print(f'> Traintime in sec: {round(np.mean(train_time),0)} (+- {round(np.std(train_time),0)})')\r\n",
    "        print(f'> Testtime in sec: {round(np.mean(test_time),0)} (+- {round(np.std(test_time,0))})')\r\n",
    "        print(f'> Totaltime in sec: {round(np.mean(t_time),0)} (+- {round(np.std(t_time),0)})')\r\n",
    "        print('-----------------------------------------------------------------------------')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Data Preparation\r\n",
    "\r\n",
    "# Start the stopwatch / counter \r\n",
    "t1_start = time.process_time() \r\n",
    "\r\n",
    "data = 'Rayyan'\r\n",
    "dirty_table, clean_table = Input(data)\r\n",
    "dirty_table, clean_table, attribute, maxlen = Structure(True,dirty_table,clean_table)\r\n",
    "df, X_roh, y = Merge(dirty_table, clean_table)\r\n",
    "tk_char, tk_attr = Dictionary(attribute,df)\r\n",
    "\r\n",
    "#Extend Attribute\r\n",
    "attribute, X, y, Drop_list = attribute_extend(attribute,df,X_roh,y)\r\n",
    "\r\n",
    "# Stop the stopwatch / counter\r\n",
    "t1_stop = time.process_time()\r\n",
    "t1_time = t1_stop-t1_start\r\n",
    "\r\n",
    "print('Time in sec: ',t1_time)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parameter for models\r\n",
    "n_classes = 2\r\n",
    "ver=0\r\n",
    "\r\n",
    "# Hyperparameter\r\n",
    "n_epochs = 120\r\n",
    "#batch_size=round((attribute.shape[0]-len(Drop_list))*n/4)\r\n",
    "batch_size=round((attribute.shape[0]-len(Drop_list))*5)\r\n",
    "batch_size_3=round(X.shape[0])\r\n",
    "\r\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.005, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\r\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.002, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\r\n",
    "\r\n",
    "emb_dim_char = round(len(tk_char.word_index)+1)\r\n",
    "emb_dim_attr = round(len(tk_attr.word_index)+1)\r\n",
    "rnn_dim = 64\r\n",
    "rnn_dim_att = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Test(10,20,'M0','DiverSet',True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Test(10,20,'M1','DiverSet',True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3074397aa67324493a4e19a94092cddfd49ffb744bae2a6319f30208afef7c2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}